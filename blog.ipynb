{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EZKL Workflow\n",
    "\n",
    "To prove the inference of a trained model using EZKL, we need to follow the steps below. As an example to illustrate the process, let's consider that we have just trained a simple perceptron model using PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "\n",
    "# MNIST dataset\n",
    "train, test = (torchvision.datasets.MNIST(\n",
    "    './data', \n",
    "    train=is_train,\n",
    "    transform=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        torchvision.transforms.Lambda(lambda x: x.view(-1))\n",
    "    ])\n",
    ") for is_train in [True, False])\n",
    "\n",
    "input_size, output_size = 28 * 28, 10\n",
    "\n",
    "# Define the model\n",
    "perceptron = nn.Sequential(\n",
    "    nn.Linear(input_size, output_size),\n",
    ")\n",
    "\n",
    "# Create a dataset and data loader\n",
    "train_loader, test_loader = (DataLoader(\n",
    "    dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=True\n",
    ") for dataset in [train, test])\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(perceptron.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "perceptron.train()\n",
    "for data, label in train_loader:\n",
    "    output = perceptron(data)\n",
    "    loss = criterion(output, label)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.16%\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "perceptron.eval()\n",
    "with torch.no_grad():\n",
    "    correct, total = 0, 0\n",
    "    for data, label in test_loader:\n",
    "        output = perceptron(data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += label.size(0)\n",
    "        correct += (predicted == label).sum().item()\n",
    "    print(f'Accuracy: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Model Conversion**\n",
    "\n",
    "Convert the trained model to the ONNX format. In this case, PyTorch provides the function torch.onnx.export. Other frameworks also have similar functions or external tools to convert models to ONNX (e.g., TensorFlow's tf2onnx). Nevertheless, Sklearn models are slighly more complicated to convert to suitable ONNX format, so we must first convert the model to a PyTorch using hummingbird.ml and then convert it to ONNX. We won't cover this process in this article but you can find more information in one of EZKL's notebooks.\n",
    "\n",
    "Before converting our model to ONNX format, we need to tell the converter the input shape of the model. This can be done by passing a dummy input tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose any valid input tensor (1st input of the test dataset)\n",
    "input_sample = next(iter(test_loader))[0][0].unsqueeze(0)\n",
    "\n",
    "torch.onnx.export(\n",
    "    perceptron,\n",
    "    input_sample,\n",
    "    \"perceptron.onnx\",\n",
    "    input_names = ['input'],             # Input and output labels to appear in the ONNX graph \n",
    "    output_names = ['output'],\n",
    "    dynamic_axes={\n",
    "        'input' : {0 : 'batch_size'},    # Variable length axes\n",
    "        'output' : {0 : 'batch_size'}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Setup**\n",
    "\n",
    "EZKL has several setup functions in their exposed API, namely gen_settings, calibrate_settings, compile_circuit, get_srs, setup, and gen_witness, we've group them together in this bullet point to describe the high level setup process. The signature of each function should be self-explanatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 6 columns for non-linearity table.\n",
      "Using 12 columns for non-linearity table.\n",
      "calibration failed extended k is too large to accommodate the quotient polynomial with logrows 6\n",
      "Using 12 columns for non-linearity table.\n",
      "calibration failed extended k is too large to accommodate the quotient polynomial with logrows 6\n",
      "calibration failed max lookup input (-430708640, 367947557) is too large\n",
      "calibration failed max lookup input (-430782343, 367916286) is too large\n",
      "calibration failed max lookup input (-861471861, 735938776) is too large\n",
      "calibration failed max lookup input (-1722893789, 1471769531) is too large\n",
      "\n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 13, param_scale: 13, scale_input_multiplier: 10) ------------->\n",
      "\n",
      "+---------------+--------------+--------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error    | median_error | max_error    | min_error     | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+---------------+--------------+--------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| 0.00003141314 | 0.0007660389 | 0.0015113354 | -0.0016593933 | 0.0008697137   | 0.0007660389     | 0.0016593933  | 0.00013160706 | 0.0000009796103    | -0.0000715173      | 0.0007769525           |\n",
      "+---------------+--------------+--------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ezkl\n",
    "import json\n",
    "\n",
    "def create_file(filename: str) -> str:\n",
    "    open(filename, 'w').close()\n",
    "    return filename\n",
    "\n",
    "# We have to create empty files manually before running the setup\n",
    "INPUT = create_file(\"input_data.json\")\n",
    "SETTINGS = create_file(\"settings.json\")\n",
    "CALIBRATION = create_file(\"calibration.json\")\n",
    "WITNESS = create_file(\"witness.json\")\n",
    "COMPILED_MODEL = create_file(\"compiled_model.json\")\n",
    "VK = create_file(\"vk.json\")\n",
    "PK = create_file(\"pk.json\")\n",
    "PROOF = create_file(\"proof.pf\")\n",
    "\n",
    "def setup(model, onnx_file, input_sample):\n",
    "\n",
    "    # Save the input data to a file in the expected format\n",
    "    input_data = {\n",
    "        'input_shapes': list(input_sample.shape),\n",
    "        'input_data': input_sample.detach().numpy().tolist(),\n",
    "        \"output_data\": model(input_sample).detach().numpy().tolist()\n",
    "    }\n",
    "\n",
    "    json.dump(\n",
    "        input_data,\n",
    "        open(INPUT, 'w')\n",
    "    )\n",
    "\n",
    "    # Run each setup function and verify that it succeeded\n",
    "    assert ezkl.gen_settings(\n",
    "        onnx_file,\n",
    "        SETTINGS\n",
    "    )\n",
    "\n",
    "\n",
    "    json.dump(\n",
    "        input_data,\n",
    "        open(CALIBRATION, 'w')\n",
    "    )\n",
    "\n",
    "    assert ezkl.calibrate_settings(\n",
    "        INPUT,\n",
    "        onnx_file,\n",
    "        SETTINGS, \n",
    "        \"resources\"\n",
    "    )\n",
    "\n",
    "    assert ezkl.compile_circuit(\n",
    "        onnx_file,\n",
    "        COMPILED_MODEL,\n",
    "        SETTINGS\n",
    "    )\n",
    "\n",
    "    assert ezkl.get_srs(\n",
    "        SETTINGS\n",
    "    )\n",
    "\n",
    "    ezkl.gen_witness(\n",
    "        \"input_data.json\",\n",
    "        COMPILED_MODEL,\n",
    "        WITNESS\n",
    "    )\n",
    "\n",
    "    assert ezkl.setup(\n",
    "        COMPILED_MODEL,\n",
    "        VK,\n",
    "        PK\n",
    "    )\n",
    "\n",
    "setup(perceptron, \"perceptron.onnx\", input_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Proof Generation**: Generate the proof using the `gen_proof` function. This function takes the arithmetization of the model, the witness, the public key, and the proof file as inputs and writes the proof to the specified file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instances': [['73c5f88893f5e1439170b97948e833285d588181b64550b829a031e1724e6430',\n",
      "                '0554ec0000000000000000000000000000000000000000000000000000000000',\n",
      "                '8d62cde493f5e1439170b97948e833285d588181b64550b829a031e1724e6430',\n",
      "                'e42d5bee93f5e1439170b97948e833285d588181b64550b829a031e1724e6430',\n",
      "                '2a8d65e693f5e1439170b97948e833285d588181b64550b829a031e1724e6430',\n",
      "                '5007cf0600000000000000000000000000000000000000000000000000000000',\n",
      "                '8b978faf93f5e1439170b97948e833285d588181b64550b829a031e1724e6430',\n",
      "                'fbe4369793f5e1439170b97948e833285d588181b64550b829a031e1724e6430',\n",
      "                'dbb7bf5700000000000000000000000000000000000000000000000000000000',\n",
      "                '03fda0d793f5e1439170b97948e833285d588181b64550b829a031e1724e6430']],\n",
      " 'proof': '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',\n",
      " 'transcript_type': 'EVM'}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "proof = ezkl.prove(\n",
    "    WITNESS,\n",
    "    COMPILED_MODEL,\n",
    "    PK,\n",
    "    PROOF,\n",
    "    \"single\",\n",
    ")\n",
    "\n",
    "pprint.pprint(proof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Proof Verification**: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ezkl.verify(\n",
    "    PROOF,\n",
    "    SETTINGS,\n",
    "    VK\n",
    ") == True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Trend Forecasting\n",
    "\n",
    "We adapted code from one of [GIZA's examples](https://github.com/gizatechxyz/Giza-Hub/tree/token_trend_action/awesome-giza-actions/trend_token_prediction) the idea is to train multiple models with different accuracies and then compare the costs of proving the inference of each model. We first \n",
    "explain the feature extraction process in detail, which is not explained in the original example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "We will use the [Giza's dataset hub](https://github.com/gizatechxyz/datasets), which contains a collection of datasets that are relevant for blockchain applications. These datasets are publicly available and can be loaded using the `DatasetsLoader` class from the `giza_datasets` package, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset read from cache.\n",
      "Loading dataset tokens-daily-prices-mcap-volume from cache.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "polars.config.Config"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from giza_datasets import DatasetsLoader\n",
    "import polars as pl\n",
    "\n",
    "# Load the desired dataset\n",
    "DatasetsLoader().load(\"tokens-daily-prices-mcap-volume\")\n",
    "\n",
    "# For pretty printing\n",
    "pl.Config.set_tbl_hide_column_data_types(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Token Daily Price Data\n",
    "Contains daily price data (price, market capitalization, and volume) for a set of tokens (e.g., WBTC, WETH, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the dataset:\n",
      "Dataset read from cache.\n",
      "Loading dataset tokens-daily-prices-mcap-volume from cache.\n",
      "shape: (3, 5)\n",
      "┌────────────┬─────────────┬────────────┬──────────────────┬───────┐\n",
      "│ date       ┆ price       ┆ market_cap ┆ volumes_last_24h ┆ token │\n",
      "╞════════════╪═════════════╪════════════╪══════════════════╪═══════╡\n",
      "│ 2019-02-01 ┆ 3438.360403 ┆ 0.0        ┆ 20589.040403     ┆ WBTC  │\n",
      "│ 2019-02-02 ┆ 3472.243307 ┆ 0.0        ┆ 12576.723906     ┆ WBTC  │\n",
      "│ 2019-02-03 ┆ 3461.058341 ┆ 0.0        ┆ 1852.526033      ┆ WBTC  │\n",
      "└────────────┴─────────────┴────────────┴──────────────────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "print(\"First few rows of the dataset:\")\n",
    "print(DatasetsLoader().load('tokens-daily-prices-mcap-volume').head(n = 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Top APY per protocol\n",
    "Contains the top Annual Percentage Yield (APY) for each protocol in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the dataset:\n",
      "Dataset read from cache.\n",
      "Loading dataset top-pools-apy-per-protocol from cache.\n",
      "shape: (3, 6)\n",
      "┌────────────┬──────────┬─────┬─────────┬──────────────────┬──────────┐\n",
      "│ date       ┆ tvlUsd   ┆ apy ┆ project ┆ underlying_token ┆ chain    │\n",
      "╞════════════╪══════════╪═════╪═════════╪══════════════════╪══════════╡\n",
      "│ 2022-02-28 ┆ 12808    ┆ 0.0 ┆ aave-v2 ┆ STETH            ┆ Ethereum │\n",
      "│ 2022-03-01 ┆ 46045250 ┆ 0.0 ┆ aave-v2 ┆ STETH            ┆ Ethereum │\n",
      "│ 2022-03-02 ┆ 90080754 ┆ 0.0 ┆ aave-v2 ┆ STETH            ┆ Ethereum │\n",
      "└────────────┴──────────┴─────┴─────────┴──────────────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "print(\"First few rows of the dataset:\")\n",
    "print(DatasetsLoader().load('top-pools-apy-per-protocol').head(n = 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TVL per project tokens\n",
    "Contains the Total Value Locked (TVL) for each project in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the dataset:\n",
      "Dataset read from cache.\n",
      "Loading dataset tvl-per-project-tokens from cache.\n",
      "shape: (3, 47)\n",
      "┌───────┬──────┬────────┬──────┬───┬──────┬──────┬────────────┬─────────┐\n",
      "│ 1INCH ┆ AAVE ┆ AAVE.E ┆ AMPL ┆ … ┆ YFI  ┆ ZRX  ┆ date       ┆ project │\n",
      "╞═══════╪══════╪════════╪══════╪═══╪══════╪══════╪════════════╪═════════╡\n",
      "│ null  ┆ null ┆ null   ┆ null ┆ … ┆ null ┆ null ┆ 2020-11-29 ┆ aave-v2 │\n",
      "│ null  ┆ null ┆ null   ┆ null ┆ … ┆ null ┆ null ┆ 2020-11-30 ┆ aave-v2 │\n",
      "│ null  ┆ null ┆ null   ┆ null ┆ … ┆ null ┆ null ┆ 2020-12-01 ┆ aave-v2 │\n",
      "└───────┴──────┴────────┴──────┴───┴──────┴──────┴────────────┴─────────┘\n"
     ]
    }
   ],
   "source": [
    "print(\"First few rows of the dataset:\")\n",
    "print(DatasetsLoader().load('tvl-per-project-tokens').head(n = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset read from cache.\n",
      "Loading dataset tokens-daily-prices-mcap-volume from cache.\n",
      "First few rows of the dataset:\n",
      "shape: (3, 19)\n",
      "┌────────────┬─────────┬────────────┬──────────────────┬───┬───────────────┬─────┬───────┬──────┐\n",
      "│ date       ┆ price   ┆ market_cap ┆ volumes_last_24h ┆ … ┆ trend_30_days ┆ day ┆ month ┆ year │\n",
      "╞════════════╪═════════╪════════════╪══════════════════╪═══╪═══════════════╪═════╪═══════╪══════╡\n",
      "│ 2018-02-14 ┆ 839.535 ┆ 0.0        ┆ 54776.5          ┆ … ┆ null          ┆ 3   ┆ 2     ┆ 2018 │\n",
      "│ 2018-02-15 ┆ 947.358 ┆ 0.0        ┆ 111096.0         ┆ … ┆ null          ┆ 4   ┆ 2     ┆ 2018 │\n",
      "│ 2018-02-16 ┆ 886.961 ┆ 0.0        ┆ 57731.7          ┆ … ┆ null          ┆ 5   ┆ 2     ┆ 2018 │\n",
      "└────────────┴─────────┴────────────┴──────────────────┴───┴───────────────┴─────┴───────┴──────┘\n",
      "Number of rows in the dataset: 2173\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "TOKEN = \"WETH\"\n",
    "LAG = 1\n",
    "DAYS = [1, 3, 7, 15, 30]\n",
    "\n",
    "token_data = DatasetsLoader().load('tokens-daily-prices-mcap-volume')\n",
    "\n",
    "# Filter the dataset to only include the target token\n",
    "# Add a column with the labels for the target token\n",
    "# Add columns with the price difference over the specified DAYS\n",
    "# Expand the date column into day_of_week, month_of_year, and year\n",
    "target_token_price_trend = token_data \\\n",
    "    .filter(pl.col(\"token\") == TOKEN) \\\n",
    "    .with_columns(\n",
    "        ((pl.col(\"price\").shift(-LAG) - pl.col(\"price\")) > 0).cast(pl.Int8).alias(\"label\")\n",
    "    ) \\\n",
    "    .with_columns(\n",
    "        pl.col(\"price\").diff(n = days).alias(f\"price_diff_{days}_days\")\n",
    "        for days in DAYS\n",
    "    ) \\\n",
    "    .with_columns(\n",
    "        (pl.col(\"price\") - pl.col(\"price\").shift(days) > 0).cast(pl.Int8).alias(f\"trend_{days}_days\")\n",
    "        for days in DAYS\n",
    "    ) \\\n",
    "    .with_columns([\n",
    "        pl.col(\"date\").dt.weekday().alias(\"day\"),\n",
    "        pl.col(\"date\").dt.month().alias(\"month\"),\n",
    "        pl.col(\"date\").dt.year().alias(\"year\")\n",
    "    ])\n",
    "\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(target_token_price_trend.head(n = 3))\n",
    "print(\"Number of rows in the dataset:\", len(target_token_price_trend))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_data = DatasetsLoader().load('tokens-daily-prices-mcap-volume')\n",
    "correlations = {}\n",
    "\n",
    "# List all tokens in the dataset\n",
    "tokens = token_data.get_column(\"token\").unique().to_list()\n",
    "\n",
    "# Calculate the correlation between the target token and all other tokens\n",
    "for token_1, token_2 in itertools.permutations(tokens, r=2):\n",
    "    \n",
    "    # Filter the dataset and get the price and date columns\n",
    "    token_1_data = token_data.filter(pl.col(\"token\") == token_1) \\\n",
    "        .select([\"date\", \"price\"])\n",
    "    token_2_data = token_data.filter(pl.col(\"token\") == token_2) \\\n",
    "        .select([\"date\", \"price\"])\n",
    "    \n",
    "    # Join the datasets on the date column\n",
    "    joined_data = token_1_data.join(token_2_data, on=\"date\", suffix=\"_compare\")\n",
    "\n",
    "    # Nested dictionary to store the correlation between the two tokens\n",
    "    correlations[token_1] = correlations.get(token_1, {}) \n",
    "    correlations[token_1][token_2] = correlations[token_1] \\\n",
    "        .get(token_2, {\n",
    "            day: joined_data \\\n",
    "                    .with_columns(pl.col(\"price_compare\").shift(day)) \\\n",
    "                    .select(pl.corr(\"price\", \"price_compare\").alias(\"correlation\")) \\\n",
    "                    .get_column(\"correlation\")[0]\n",
    "            for day in DAYS\n",
    "        })\n",
    "\n",
    "\n",
    "#pprint.pprint(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the dataset:\n",
      "shape: (3, 29)\n",
      "┌────────────┬─────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
      "│ date       ┆ price   ┆ market_ca ┆ volumes_l ┆ … ┆ price_tre ┆ price_tre ┆ price_tre ┆ price_tre │\n",
      "│            ┆         ┆ p         ┆ ast_24h   ┆   ┆ nd_ETH_3  ┆ nd_ETH_7  ┆ nd_ETH_15 ┆ nd_ETH_30 │\n",
      "╞════════════╪═════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
      "│ 2018-02-14 ┆ 839.535 ┆ 0.0       ┆ 54776.5   ┆ … ┆ 0         ┆ 1         ┆ 0         ┆ 0         │\n",
      "│ 2018-02-15 ┆ 947.358 ┆ 0.0       ┆ 111096.0  ┆ … ┆ 1         ┆ 1         ┆ 0         ┆ 0         │\n",
      "│ 2018-02-16 ┆ 886.961 ┆ 0.0       ┆ 57731.7   ┆ … ┆ 1         ┆ 1         ┆ 0         ┆ 0         │\n",
      "└────────────┴─────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "K = 10\n",
    "\n",
    "# Dataframe to store the final results\n",
    "price_dataset = target_token_price_trend\n",
    "\n",
    "# Retrive the relevant data from the nested dictionary\n",
    "target_token_correlations = correlations[TOKEN]\n",
    "\n",
    "# Get the top K correlated tokens for each lag\n",
    "top_k_correlated_tokens_by_lag = {\n",
    "    lag: sorted(target_token_correlations.items(), key=lambda x: x[1][lag], reverse=True)[:K]\n",
    "    for lag in DAYS\n",
    "}\n",
    "\n",
    "top_k_correlated_tokens_15_days = top_k_correlated_tokens_by_lag[15]\n",
    "\n",
    "for token, _ in top_k_correlated_tokens_15_days:\n",
    "\n",
    "    # Column names for the price differences\n",
    "    price_diff_columns = [f\"price_diff_{token}_{days}\" for days in DAYS]\n",
    "    price_trend_columns = [f\"price_trend_{token}_{days}\" for days in DAYS]\n",
    "\n",
    "    # Filter the dataset to only include the correlated token\n",
    "    token_prices = token_data.filter(pl.col(\"token\") == token)\n",
    "\n",
    "    # Add columns with the price differences for each day\n",
    "    token_prices = token_prices \\\n",
    "        .with_columns(\n",
    "            pl.col(\"price\").diff(n = days).alias(tag)\n",
    "            for days, tag in zip(DAYS, price_diff_columns)\n",
    "        ) \\\n",
    "        .with_columns([\n",
    "            (pl.col(\"price\") - pl.col(\"price\").shift(days) > 0).cast(pl.Int8).alias(tag)\n",
    "            for days, tag in zip(DAYS, price_trend_columns)\n",
    "        ]) \\\n",
    "        .select([\"date\"] + price_diff_columns + price_trend_columns)\n",
    "\n",
    "    # Join the dataset with the target token dataset\n",
    "    price_dataset = price_dataset.join(token_prices, on=\"date\", how=\"left\")\n",
    "    break\n",
    "\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(price_dataset.head(n = 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset read from cache.\n",
      "Loading dataset top-pools-apy-per-protocol from cache.\n",
      "First few rows of the dataset:\n",
      "shape: (3, 91)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
      "│ date      ┆ tvlUsd_pr ┆ tvlUsd_pr ┆ tvlUsd_pr ┆ … ┆ apy_proje ┆ apy_proje ┆ apy_proje ┆ apy_proj │\n",
      "│           ┆ oject_aav ┆ oject_aav ┆ oject_aav ┆   ┆ ct_uniswa ┆ ct_uniswa ┆ ct_uniswa ┆ ect_year │\n",
      "│           ┆ e-v2_Ethe ┆ e-v2_Poly ┆ e-v2_Aval ┆   ┆ p-v3_Arbi ┆ p-v3_Arbi ┆ p-v3_Ethe ┆ n-financ │\n",
      "│           ┆ reumW…    ┆ gonWE…    ┆ anche…    ┆   ┆ trumW…    ┆ trumW…    ┆ reumW…    ┆ e_Ethere │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ …        │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
      "│ 2022-02-1 ┆ 246215633 ┆ 560180650 ┆ 719972444 ┆ … ┆ null      ┆ null      ┆ null      ┆ null     │\n",
      "│ 1         ┆ 5         ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 2022-02-1 ┆ 246420416 ┆ 537846447 ┆ 672831429 ┆ … ┆ null      ┆ null      ┆ null      ┆ null     │\n",
      "│ 2         ┆ 1         ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 2022-02-1 ┆ 249160040 ┆ 508550871 ┆ 669386509 ┆ … ┆ null      ┆ null      ┆ null      ┆ null     │\n",
      "│ 3         ┆ 0         ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘\n",
      "Number of rows in the dataset: 711\n"
     ]
    }
   ],
   "source": [
    "top_apy_per_protocol = DatasetsLoader().load(\"top-pools-apy-per-protocol\")\n",
    "\n",
    "START_DATE = pl.datetime(2022, 6, 1)\n",
    "\n",
    "# Filter the dataset to only include protocols with the target token\n",
    "\"\"\" unique_token_projects = top_apy_per_protocol \\\n",
    "    .filter(pl.col(\"underlying_token\").str.contains(TOKEN)) \\\n",
    "    .filter(pl.col(\"date\") > START_DATE) \\\n",
    "    .unique(\"project\") \\\n",
    "    .pivot(index=\"date\", columns=\"project\", values=[\"apy\", \"tvlUsd\"]) \"\"\"\n",
    "\n",
    "apy_df = top_apy_per_protocol \\\n",
    "    .filter(pl.col(\"underlying_token\").str.contains(TOKEN)) \\\n",
    "    .with_columns(\n",
    "            pl.col(\"project\") + \"_\" + pl.col(\"chain\") +  pl.col(\"underlying_token\")\n",
    "    ) \\\n",
    "    .drop([\"underlying_token\", \"chain\"])\n",
    "\n",
    "unique_projects = apy_df \\\n",
    "    .filter(pl.col(\"date\") <= START_DATE) \\\n",
    "    .select(\"project\") \\\n",
    "    .unique()\n",
    "\n",
    "apy_df_token = apy_df.join(\n",
    "    unique_projects, \n",
    "    on=\"project\", \n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "unique_token_projects = apy_df_token.pivot(\n",
    "    index=\"date\",\n",
    "    columns=\"project\",\n",
    "    values=[\"tvlUsd\", \"apy\"]\n",
    ")\n",
    "\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(unique_token_projects.head(n = 3))\n",
    "print(\"Number of rows in the dataset:\", len(unique_token_projects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset read from cache.\n",
      "Loading dataset tvl-per-project-tokens from cache.\n",
      "First few rows of the dataset:\n",
      "shape: (3, 20)\n",
      "┌────────────┬──────────┬──────────┬──────┬───┬───────────┬────────────┬────────────┬──────────────┐\n",
      "│ date       ┆ aave-v2  ┆ aave-v3  ┆ aura ┆ … ┆ sushiswap ┆ uniswap-v2 ┆ uniswap-v3 ┆ yearn-financ │\n",
      "│            ┆          ┆          ┆      ┆   ┆           ┆            ┆            ┆ e            │\n",
      "╞════════════╪══════════╪══════════╪══════╪═══╪═══════════╪════════════╪════════════╪══════════════╡\n",
      "│ 2022-06-12 ┆ 1.1741e9 ┆ 3.1182e8 ┆ null ┆ … ┆ null      ┆ null       ┆ 1.0583e9   ┆ null         │\n",
      "│ 2022-07-06 ┆ 6.5735e8 ┆ 1.2864e8 ┆ null ┆ … ┆ 3.9497e7  ┆ null       ┆ 7.8917e8   ┆ null         │\n",
      "│ 2022-07-07 ┆ 6.3625e8 ┆ 1.3310e8 ┆ null ┆ … ┆ 4.1564e7  ┆ null       ┆ 7.7908e8   ┆ null         │\n",
      "└────────────┴──────────┴──────────┴──────┴───┴───────────┴────────────┴────────────┴──────────────┘\n"
     ]
    }
   ],
   "source": [
    "tvl_df = DatasetsLoader().load(\"tvl-per-project-tokens\") \\\n",
    "    .unique(subset=[\"date\", \"project\"]) \\\n",
    "    .filter(pl.col(\"date\") > START_DATE) \n",
    "\n",
    "tvl_per_projects_token = tvl_df[[TOKEN, \"project\", \"date\"]].pivot(\n",
    "    index=\"date\",\n",
    "    columns=\"project\",\n",
    "    values=TOKEN\n",
    ")\n",
    "\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(tvl_per_projects_token.head(n = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the dataset:\n",
      "shape: (3, 138)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
      "│ date      ┆ price     ┆ market_ca ┆ volumes_l ┆ … ┆ apy_proje ┆ apy_proje ┆ apy_proje ┆ apy_proj │\n",
      "│           ┆           ┆ p         ┆ ast_24h   ┆   ┆ ct_uniswa ┆ ct_uniswa ┆ ct_uniswa ┆ ect_year │\n",
      "│           ┆           ┆           ┆           ┆   ┆ p-v3_Arbi ┆ p-v3_Arbi ┆ p-v3_Ethe ┆ n-financ │\n",
      "│           ┆           ┆           ┆           ┆   ┆ trumW…    ┆ trumW…    ┆ reumW…    ┆ e_Ethere │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ …        │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
      "│ 2022-06-0 ┆ 1828.2324 ┆ 0.0       ┆ 1.6046e9  ┆ … ┆ 4.87082   ┆ 27.33455  ┆ 0.0       ┆ 0.55534  │\n",
      "│ 2         ┆ 82        ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 2022-06-0 ┆ 1832.6794 ┆ 0.0       ┆ 1.2320e9  ┆ … ┆ 2.73812   ┆ 26.00393  ┆ 0.0       ┆ 0.50857  │\n",
      "│ 3         ┆ 96        ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 2022-06-0 ┆ 1775.8567 ┆ 0.0       ┆ 1.0539e9  ┆ … ┆ null      ┆ null      ┆ null      ┆ 0.50457  │\n",
      "│ 4         ┆ 49        ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘\n",
      "First few rows of the dataset:\n",
      "shape: (3, 122)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
      "│ volumes_l ┆ price_dif ┆ price_dif ┆ price_dif ┆ … ┆ apy_proje ┆ apy_proje ┆ apy_proje ┆ apy_proj │\n",
      "│ ast_24h   ┆ f_1_days  ┆ f_3_days  ┆ f_7_days  ┆   ┆ ct_uniswa ┆ ct_uniswa ┆ ct_uniswa ┆ ect_year │\n",
      "│           ┆           ┆           ┆           ┆   ┆ p-v3_Arbi ┆ p-v3_Arbi ┆ p-v3_Ethe ┆ n-financ │\n",
      "│           ┆           ┆           ┆           ┆   ┆ trumW…    ┆ trumW…    ┆ reumW…    ┆ e_Ethere │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ …        │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
      "│ 1.024704  ┆ -2.197109 ┆ 0.11474   ┆ -0.843464 ┆ … ┆ -0.576543 ┆ -0.54936  ┆ -0.044265 ┆ -0.92143 │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ 2        │\n",
      "│ 0.498189  ┆ 0.064754  ┆ -1.764524 ┆ 0.120361  ┆ … ┆ -0.678317 ┆ -0.580351 ┆ -0.044265 ┆ -0.94637 │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ 2        │\n",
      "│ 0.246527  ┆ -1.042419 ┆ -1.890364 ┆ 0.298075  ┆ … ┆ 0.0       ┆ 0.0       ┆ 0.0       ┆ -0.94850 │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ 5        │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "# Join the datasets by the date column to create the final dataset\n",
    "final_dataset = price_dataset \\\n",
    "    .join(tvl_per_projects_token, on=\"date\", how=\"inner\") \\\n",
    "    .join(unique_token_projects, on=\"date\", how=\"inner\")\n",
    "\n",
    "# Drop unnecessary columns and rows with irrelevant data\n",
    "# - columns with token, market_cap , date and current price: not relevant for the model\n",
    "# - rows with year < 2022: historical data is not relevant\n",
    "final_dataset = final_dataset \\\n",
    "    .filter(pl.col(\"year\") >= 2022) \\\n",
    "    .drop([\"token\", \"market_cap\", \"date\", \"price\", \"month\"])\n",
    "final_dataset = final_dataset.slice(0, len(final_dataset) - 1)\n",
    "# Drop columns if there are lots of missing values\n",
    "THRESHOLD = 0.2\n",
    "max_nulls = THRESHOLD * final_dataset.shape[0]\n",
    "columns_to_keep = [\n",
    "        col_name for col_name in final_dataset.columns if final_dataset[col_name].null_count() <= max_nulls\n",
    "]\n",
    "final_dataset = final_dataset.select(columns_to_keep    )\n",
    "\n",
    "# Split the dataset into features and labels\n",
    "features = final_dataset.drop(\"label\")\n",
    "labels = final_dataset[\"label\"]\n",
    "\n",
    "# Normalize the training dataset and fill missing values\n",
    "for col in features.columns:\n",
    "    mean_val = features[col].mean()\n",
    "    std_dev = features[col].std() if features[col].std() != 0 else 1\n",
    "    features = features.with_columns(((features[col].fill_null(mean_val) - mean_val) / std_dev).alias(col))\n",
    "\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(features.head(n = 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "##### Splitting the dataset\n",
    "\n",
    "We need to convert the dataframes to torch tensors and split the dataset into training and testing sets. As usual, we choose a reasonable split ratio (e.g., 80% training and 20% testing) and shuffle the data before splitting it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataset to a PyTorch tensor\n",
    "features_tensor = torch.tensor(features.to_numpy(), dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(labels.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "# Get a random permutation of the indices\n",
    "indices = torch.randperm(len(features_tensor))\n",
    "train_indices = indices[:int(0.75 * len(features_tensor))]\n",
    "test_indices = indices[int(0.75 * len(features_tensor)):]\n",
    "train_features, train_labels = features_tensor[train_indices], labels_tensor[train_indices]\n",
    "test_features, test_labels = features_tensor[test_indices], labels_tensor[test_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Definition\n",
    "\n",
    "Initially we use a multi-layer perceptron (MLP) model with three hidden layers and ReLU activation functions. For the final layer, we use a sigmoid activation function to output the probability of the token price increasing. We also define the loss function (mean squared error) and the optimizer (Adam) for training the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(len(features.columns), 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Model\n",
    "\n",
    "We train the model using the training dataset and evaluate its performance on the testing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 67.33%\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_features)\n",
    "    loss = criterion(output, train_labels.unsqueeze(1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct, total = 0, 0\n",
    "    output = model(test_features)\n",
    "    predicted = torch.tensor([1 if x > 0.5 else 0 for x in output])\n",
    "    total += test_labels.size(0)\n",
    "    correct += (predicted == test_labels).sum().item()\n",
    "    print(f'Accuracy: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking Proof Generation\n",
    "\n",
    "Now that we have trained the model, we can generate a proof of its inference using EZKL. We first convert the model to the ONNX format and set up the proof generation process as described in the previous sections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onnx(model, input_sample, onnx_file):\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        input_sample,\n",
    "        onnx_file,\n",
    "        input_names = ['input'],             # Input and output labels to appear in the ONNX graph \n",
    "        output_names = ['output'],\n",
    "        dynamic_axes={\n",
    "            'input' : {0 : 'batch_size'},    # Variable length axes\n",
    "            'output' : {0 : 'batch_size'}\n",
    "        }\n",
    "    )\n",
    "\n",
    "input_sample = next(iter(test_features)).unsqueeze(0)\n",
    "to_onnx(model, input_sample, \"perceptron.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also create several methods to help us with the benchmarking process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Union\n",
    "import time\n",
    "import torch\n",
    "import sklearn as sk\n",
    "\n",
    "def prove():\n",
    "    _= ezkl.prove(\n",
    "        WITNESS,\n",
    "        COMPILED_MODEL,\n",
    "        PK,\n",
    "        PROOF,\n",
    "        \"single\",\n",
    "    )\n",
    "\n",
    "def verify():\n",
    "    assert ezkl.verify(\n",
    "        PROOF,\n",
    "        SETTINGS,\n",
    "        VK,\n",
    "    ) == True\n",
    "\n",
    "def bench_ezkl_single_round(\n",
    "    model: Union[torch.nn.Module, sk.base.BaseEstimator],\n",
    "    model_onnx_file: str, \n",
    "    sample: torch.Tensor, \n",
    ") -> Tuple[float, float, float]:\n",
    "    setup_time = -time.time()\n",
    "    setup(model, model_onnx_file, sample)\n",
    "    setup_time += time.time()\n",
    "    \n",
    "    prove_time = -time.time()\n",
    "    prove()\n",
    "    prove_time += time.time()\n",
    "\n",
    "    verify_time = -time.time()\n",
    "    verify()\n",
    "    verify_time += time.time()\n",
    "\n",
    "    return setup_time, prove_time, verify_time\n",
    "\n",
    "import contextlib\n",
    "\n",
    "def bench_ezkl(\n",
    "    model: Union[torch.nn.Module, sk.base.BaseEstimator],\n",
    "    model_onnx_file: str,\n",
    "    sample: torch.Tensor,\n",
    "    rounds: int = 1\n",
    ") -> Tuple[float, float, float]:\n",
    "    setup_time, prove_time, verify_time = [], [], []\n",
    "    for _ in range(rounds):\n",
    "        with contextlib.redirect_stderr(None):\n",
    "            s, p, v = bench_ezkl_single_round(model, model_onnx_file, sample)\n",
    "        setup_time.append(s), prove_time.append(p), verify_time.append(v)\n",
    "    avg_setup, avg_prove, avg_verify = (\n",
    "        sum(setup_time) / rounds, \n",
    "        sum(prove_time) / rounds, \n",
    "        sum(verify_time) / rounds\n",
    "    )\n",
    "\n",
    "    std_setup, std_prove, std_verify = (\n",
    "        (sum((s - avg_setup) ** 2 for s in setup_time) / rounds) ** 0.5,\n",
    "        (sum((p - avg_prove) ** 2 for p in prove_time) / rounds) ** 0.5,\n",
    "        (sum((v - avg_verify) ** 2 for v in verify_time) / rounds) ** 0.5\n",
    "    )\n",
    "\n",
    "    print(f\"Setup time: {str(avg_setup)[:5]} ± {str(std_setup)[:5]} [s]\")\n",
    "    print(f\"Prover time: {str(avg_prove)[:5]} ± {str(std_prove)[:5]} [s]\")\n",
    "    print(f\"Verifier time: {str(avg_verify)[:5]} ± {str(std_verify)[:5]} [s]\")\n",
    "    \n",
    "    return setup_time, prove_time, verify_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's benchmark the proof generation process for the MLP model we trained earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup time: 6.129 ± 0.456 [s]\n",
      "Prover time: 8.216 ± 0.429 [s]\n",
      "Verifier time: 0.014 ± 0.001 [s]\n"
     ]
    }
   ],
   "source": [
    "_ = bench_ezkl(\n",
    "    model,\n",
    "    \"perceptron.onnx\",\n",
    "    input_sample,\n",
    "    rounds=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy vs. Proof Cost\n",
    "\n",
    "Let us now define a more complex model that slighly increases the accuracy of the predictions at the cost of a longer proof generation time. We will use a deeper MLP model with more hidden layers and neurons, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.33%\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(len(features.columns), 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_features)\n",
    "    loss = criterion(output, train_labels.unsqueeze(1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct, total = 0, 0\n",
    "    output = model(test_features)\n",
    "    predicted = torch.tensor([1 if x > 0.5 else 0 for x in output])\n",
    "    total += test_labels.size(0)\n",
    "    correct += (predicted == test_labels).sum().item()\n",
    "    print(f'Accuracy: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup time: 11.49 ± 0.727 [s]\n",
      "Prover time: 15.09 ± 0.341 [s]\n",
      "Verifier time: 0.021 ± 0.000 [s]\n"
     ]
    }
   ],
   "source": [
    "to_onnx(model, input_sample, \"perceptron.onnx\")\n",
    "\n",
    "_ = bench_ezkl(\n",
    "    model,\n",
    "    \"perceptron.onnx\",\n",
    "    input_sample,\n",
    "    rounds=10\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
